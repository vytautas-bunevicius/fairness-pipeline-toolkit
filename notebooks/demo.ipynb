{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fairness Pipeline Demo\n\nThis notebook shows how to detect and fix bias in machine learning models. We'll walk through a real example where hiring decisions are unfairly biased against certain groups, then show how our toolkit fixes this problem."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## The Problem We're Solving\n\nImagine you're building a model to help with hiring decisions. The model looks at applicants' age, income, education, race, and sex to predict who will be successful. But there's a problem - the model is biased. It gives higher scores to white males and lower scores to other groups.\n\nThis is a common issue in machine learning. Models often reflect the biases present in historical data, leading to unfair outcomes.\n\n## Our Solution\n\nWe built a three-step process to fix this:\n\n1. **Measure the bias** - Figure out how biased the model is\n2. **Fix the data** - Adjust the training data to be more fair  \n3. **Train a fair model** - Use special techniques to ensure the final model treats all groups fairly\n\nLet's see this in action."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## How Our Toolkit Works\n\nOur toolkit has three main parts:\n\n- **Bias Detector**: Finds unfair patterns in data and model predictions\n- **Data Transformer**: Adjusts the data to reduce bias while keeping it realistic\n- **Fair Classifier**: Trains models that are required to treat all groups fairly\n\nEverything is controlled by a simple config file that lets you adjust how much bias correction to apply."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\nFirst, let's import the tools we need and set up our environment."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T17:12:16.316560Z",
     "start_time": "2025-08-12T17:12:15.485849Z"
    }
   },
   "source": "import sys\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport warnings\nimport yaml\nfrom IPython.display import display\nimport mlflow\nfrom mlflow.tracking import MlflowClient\nfrom datetime import datetime\nimport hashlib\nimport json\n\nfrom fairness_pipeline_toolkit.pipeline_executor import PipelineExecutor\nfrom fairness_pipeline_toolkit.config import ConfigParser\nfrom fairness_pipeline_toolkit.measurement.bias_detector import BiasDetector\nfrom fairness_pipeline_toolkit.pipeline.bias_mitigation_transformer import BiasMitigationTransformer\nfrom fairness_pipeline_toolkit.training.fairness_constrained_classifier import FairnessConstrainedClassifier\n\nwarnings.filterwarnings('ignore')\nplt.style.use('default')\nsns.set_palette(\"husl\")\npd.set_option('display.max_columns', None)\npd.set_option('display.precision', 4)\n\nnotebook_dir = Path().resolve()\nparent_dir = notebook_dir.parent\nsrc_path = parent_dir / \"src\"\nsys.path.insert(0, str(src_path))\n\nprint(f\"Project Directory: {parent_dir}\")\nprint(f\"Source Path: {src_path}\")\nprint(f\"Python Version: {sys.version_info.major}.{sys.version_info.minor}\")\nprint(\"Environment initialized successfully\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Directory: /Users/vytautasbunevicius/fairness-pipeline-toolkit\n",
      "Source Path: /Users/vytautasbunevicius/fairness-pipeline-toolkit/src\n",
      "Python Version: 3.13\n",
      "Environment initialized successfully\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Now let's import our fairness toolkit components:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T17:12:16.426055Z",
     "start_time": "2025-08-12T17:12:16.386754Z"
    }
   },
   "source": "print(\"Successfully imported our fairness toolkit:\")\nprint(\"- PipelineExecutor: Runs the complete bias-fixing process\")\nprint(\"- BiasDetector: Finds unfair patterns in data\")\nprint(\"- BiasMitigationTransformer: Fixes biased data\")\nprint(\"- FairnessConstrainedClassifier: Trains fair models\")\n\nprint(\"\\nThese three components work together:\")\nprint(\"1. Detect bias in your data and models\")\nprint(\"2. Transform the data to reduce bias\")\nprint(\"3. Train models with fairness requirements\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Configuration\n\nLet's look at our config file, which controls how the bias-fixing process works:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T17:12:16.617471Z",
     "start_time": "2025-08-12T17:12:16.508326Z"
    }
   },
   "source": "config_path = parent_dir / \"config.yml\"\nconfig = ConfigParser.load(config_path)\n\nprint(\"Our configuration settings:\")\n\nprint(\"\\nData we're working with:\")\ndata_config = config['data']\nprint(f\"  Target: {data_config['target_column']} (what we're predicting)\")\nprint(f\"  Protected groups: {', '.join(data_config['sensitive_features'])}\")\nprint(f\"  Train/test split: {int((1-data_config['test_size'])*100)}/{int(data_config['test_size']*100)}\")\n\nprint(\"\\nBias reduction settings:\")\ntransformer_config = config['preprocessing']['transformer']\nrepair_level = transformer_config['parameters']['repair_level']\nprint(f\"  Method: {transformer_config['name']}\")\nprint(f\"  Repair level: {repair_level} ({int(repair_level*100)}% bias correction)\")\n\nprint(\"\\nFair training settings:\")\ntraining_config = config['training']['method']\nprint(f\"  Algorithm: {training_config['name']}\")\nprint(f\"  Base model: {training_config['parameters']['base_estimator']}\")\nprint(f\"  Fairness rule: {training_config['parameters']['constraint']}\")\n\nprint(\"\\nSuccess criteria:\")\neval_config = config['evaluation']\nprint(f\"  Main goal: Reduce {eval_config['primary_metric'].replace('_', ' ')}\")\nprint(f\"  Target: Get below {eval_config['fairness_threshold']} difference between groups\")\n\nprint(\"\\nWhat this means:\")\nprint(f\"- We'll reduce bias by {int(repair_level*100)}% in the data\")\nprint(f\"- Then train a LogisticRegression model with demographic parity constraints\")\nprint(f\"- Success = difference between group rates ≤ {eval_config['fairness_threshold']}\")\nprint(f\"- All experiments get tracked automatically for reproducibility\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Our Dataset\n\nSince we don't have real hiring data, we'll create a realistic dataset that shows the bias problems we're trying to solve."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T17:12:17.127806Z",
     "start_time": "2025-08-12T17:12:16.645358Z"
    }
   },
   "source": "executor = PipelineExecutor(config, verbose=False, enable_logging=False)\nsynthetic_data = executor._generate_synthetic_data(n_samples=1000)\n\nprint(\"Created a realistic hiring dataset:\")\nprint(f\"  {synthetic_data.shape[0]} job applicants\")\nprint(f\"  {synthetic_data.shape[1]} features per person\")\n\nprint(\"\\nFeatures in our dataset:\")\nfor col in synthetic_data.columns:\n    if col != 'target':\n        if synthetic_data[col].dtype in ['int64', 'float64']:\n            mean_val = synthetic_data[col].mean()\n            print(f\"  {col}: average = {mean_val:.1f}\")\n        else:\n            counts = synthetic_data[col].value_counts()\n            top_group = counts.index[0]\n            print(f\"  {col}: {len(counts)} groups, most common = {top_group} ({counts[top_group]} people)\")\n\nprint(\"\\nWhat makes this dataset realistic:\")\nprint(\"- Income varies significantly (some people earn much more)\")\nprint(\"- Race and sex affect income (historical bias)\")\nprint(\"- Education and income are correlated\")\nprint(\"- Some combinations (like White males) have advantages\")\n\nprint(\"\\nHere's what the first few applicants look like:\")\ndisplay(synthetic_data.head())",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Finding the Bias\n\nLet's see how biased our hiring data is by looking at success rates across different groups:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T17:12:18.366086Z",
     "start_time": "2025-08-12T17:12:17.319892Z"
    }
   },
   "source": "race_success_rates = synthetic_data.groupby('race')['target'].mean().sort_values(ascending=False)\nsex_success_rates = synthetic_data.groupby('sex')['target'].mean().sort_values(ascending=False)\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nbars = plt.bar(range(len(race_success_rates)), race_success_rates.values, color='skyblue')\nplt.title('Success Rate by Race')\nplt.ylabel('Hiring Success Rate')\nplt.xticks(range(len(race_success_rates)), race_success_rates.index, rotation=45)\nplt.ylim(0, 1)\nfor i, bar in enumerate(bars):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n             f'{race_success_rates.values[i]:.2f}', ha='center', va='bottom')\n\nplt.subplot(1, 3, 2)\nbars = plt.bar(range(len(sex_success_rates)), sex_success_rates.values, color='lightcoral')\nplt.title('Success Rate by Sex')\nplt.ylabel('Hiring Success Rate')\nplt.xticks(range(len(sex_success_rates)), sex_success_rates.index)\nplt.ylim(0, 1)\nfor i, bar in enumerate(bars):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n             f'{sex_success_rates.values[i]:.2f}', ha='center', va='bottom')\n\nplt.subplot(1, 3, 3)\ncombined_rates = synthetic_data.groupby(['race', 'sex'])['target'].mean().unstack()\ncombined_rates.plot(kind='bar', ax=plt.gca(), color=['lightcoral', 'skyblue'])\nplt.title('Success Rate by Race and Sex')\nplt.ylabel('Hiring Success Rate')\nplt.xticks(rotation=45)\nplt.legend(title='Sex')\n\nplt.tight_layout()\nplt.show()\n\noverall_rate = synthetic_data['target'].mean()\nrace_gap = race_success_rates.max() - race_success_rates.min()\nsex_gap = abs(sex_success_rates.iloc[0] - sex_success_rates.iloc[1])\n\ncombined_rates_flat = synthetic_data.groupby(['race', 'sex'])['target'].mean()\nmax_rate = combined_rates_flat.max()\nmin_rate = combined_rates_flat.min()\nbiggest_gap = max_rate - min_rate\n\nprint(f\"Bias Analysis Results:\")\nprint(f\"  Overall hiring rate: {overall_rate:.2f} ({int(overall_rate*100)}% of applicants hired)\")\nprint(f\"  Gap between racial groups: {race_gap:.2f}\")\nprint(f\"  Gap between sexes: {sex_gap:.2f}\")\nprint(f\"  Biggest gap (race + sex): {biggest_gap:.2f}\")\nprint(f\"  Our fairness goal: gaps ≤ {config['evaluation']['fairness_threshold']}\")\n\nprint(f\"\\nThe Problem:\")\nhighest_group = combined_rates_flat.idxmax()\nlowest_group = combined_rates_flat.idxmin()\nprint(f\"  Highest success rate: {highest_group[0]} {highest_group[1]} ({combined_rates_flat[highest_group]:.2f})\")\nprint(f\"  Lowest success rate: {lowest_group[0]} {lowest_group[1]} ({combined_rates_flat[lowest_group]:.2f})\")\nprint(f\"  This {biggest_gap:.2f} difference is much bigger than our {config['evaluation']['fairness_threshold']} goal\")\nprint(f\"\\nNext: Our toolkit will fix this bias problem\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Running the Fairness Pipeline\n\nNow we'll run our three-step process to fix the bias:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T17:12:39.483815Z",
     "start_time": "2025-08-12T17:12:18.587993Z"
    }
   },
   "source": "print(\"Running the Fairness Pipeline:\")\nprint(\"  Step 1: Measure how biased our original model is\")\nprint(\"  Step 2: Fix the data to reduce bias (80% repair)\")\nprint(\"  Step 3: Train a new model with fairness constraints\")\nprint(\"  Step 4: Check how much we improved\")\n\nexecutor = PipelineExecutor(config, verbose=True, enable_logging=False)\nresults = executor.execute_pipeline()\n\nprint(\"\\nPipeline completed successfully!\")\nprint(\"Results ready for analysis\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Results: Did We Fix the Bias?\n\nLet's compare how our model performed before and after applying fairness fixes:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T17:12:40.296468Z",
     "start_time": "2025-08-12T17:12:40.271418Z"
    }
   },
   "source": "baseline_metrics = results['baseline_report']['prediction_audit']['metrics']\nfinal_metrics = results['final_report']['metrics']\nthreshold = config['evaluation']['fairness_threshold']\n\nprint(\"Before vs After Comparison:\")\nprint(\"=\" * 50)\n\nprint(\"Model Accuracy (how often it's correct):\")\nbaseline_accuracy = baseline_metrics.get('accuracy', 0)\nfinal_accuracy = final_metrics.get('accuracy', 0)\naccuracy_change = final_accuracy - baseline_accuracy\nprint(f\"  Before: {baseline_accuracy:.1%}\")\nprint(f\"  After:  {final_accuracy:.1%}\")\nprint(f\"  Change: {accuracy_change:+.1%}\")\n\nprint(\"\\nFairness Metrics (lower = more fair):\")\nfairness_metrics = ['demographic_parity_difference', 'equalized_odds_difference']\nfor metric in fairness_metrics:\n    if metric in baseline_metrics and metric in final_metrics:\n        baseline_val = baseline_metrics[metric]\n        final_val = final_metrics[metric]\n        improvement = baseline_val - final_val\n        metric_name = metric.replace('_', ' ').replace('difference', 'gap')\n        print(f\"  {metric_name}:\")\n        print(f\"    Before: {baseline_val:.3f}\")\n        print(f\"    After:  {final_val:.3f}\")\n        print(f\"    Improvement: {improvement:+.3f}\")\n\nprimary_metric = config['evaluation']['primary_metric']\nif primary_metric in baseline_metrics and primary_metric in final_metrics:\n    baseline_primary = baseline_metrics[primary_metric]\n    final_primary = final_metrics[primary_metric]\n    improvement = baseline_primary - final_primary\n    \n    print(f\"\\nOur Main Goal ({primary_metric.replace('_', ' ')}):\")\n    print(f\"  Before: {baseline_primary:.3f}\")\n    print(f\"  After:  {final_primary:.3f}\")\n    print(f\"  Improvement: {improvement:+.3f}\")\n    print(f\"  Target: ≤ {threshold}\")\n    \n    if final_primary <= threshold:\n        print(f\"  Result: SUCCESS! We reached our fairness goal\")\n    elif improvement > 0:\n        print(f\"  Result: IMPROVED but not quite there yet\")\n    else:\n        print(f\"  Result: FAILED - no improvement\")\n\nprint(f\"\\nSummary:\")\nif final_accuracy >= baseline_accuracy - 0.01:  # Allow small accuracy drop\n    print(f\"✓ Maintained good model accuracy ({final_accuracy:.1%})\")\nelse:\n    print(f\"⚠ Model accuracy dropped significantly\")\n\nif improvement > 0:\n    print(f\"✓ Reduced bias in hiring decisions\")\n    print(f\"✓ Model now treats different groups more fairly\")\nelse:\n    print(f\"✗ Did not reduce bias as expected\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Experiment Tracking\n\nEverything we just did was automatically tracked for reproducibility:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T17:12:40.740726Z",
     "start_time": "2025-08-12T17:12:40.516105Z"
    }
   },
   "source": "experiment_name = config['mlflow']['experiment_name']\nexperiment = mlflow.get_experiment_by_name(experiment_name)\n\nprint(\"Automatic Experiment Tracking:\")\n\nif experiment:\n    print(f\"  Experiment: {experiment_name}\")\n    \n    client = MlflowClient()\n    runs = client.search_runs(\n        experiment_ids=[experiment.experiment_id],\n        max_results=3,\n        order_by=[\"start_time DESC\"]\n    )\n    \n    if runs:\n        latest_run = runs[0]\n        \n        print(f\"  Latest Run: {latest_run.info.run_id[:8]}...\")\n        print(f\"  Status: {latest_run.info.status}\")\n        print(f\"  Time: {datetime.fromtimestamp(latest_run.info.start_time/1000).strftime('%H:%M:%S')}\")\n        \n        print(f\"\\n  What got tracked automatically:\")\n        metrics_tracked = len(latest_run.data.metrics)\n        params_tracked = len(latest_run.data.params)\n        print(f\"    - {metrics_tracked} performance metrics\")\n        print(f\"    - {params_tracked} configuration parameters\")\n        print(f\"    - Trained model with metadata\")\n        print(f\"    - Configuration file\")\n        print(f\"    - Data transformation details\")\n        \n        print(f\"\\n  To see everything: Run 'mlflow ui' in the project folder\")\n        \nelse:\n    print(f\"  No experiment found (this is unusual)\")\n\nprint(f\"\\nWhy this matters:\")\nprint(f\"- Someone else can reproduce your exact results\")\nprint(f\"- You can compare different approaches easily\")\nprint(f\"- Models can be deployed directly from the tracking system\")\nprint(f\"- Full audit trail of what changes improved fairness\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## What We Accomplished\n\nThis demo showed how to systematically detect and reduce bias in machine learning models:\n\n**The Process:**\n1. **Started with biased data** - Some groups had unfair advantages in hiring\n2. **Applied bias correction** - Used 80% data repair to level the playing field\n3. **Trained a fair model** - Added constraints to ensure equal treatment\n4. **Verified the improvement** - Checked that bias was actually reduced\n\n**Key Benefits:**\n- **Automated process** - Just configure and run, no manual intervention\n- **Maintains accuracy** - Fairness improvements don't destroy model performance  \n- **Fully tracked** - Every experiment is saved for reproducibility\n- **Easy to adjust** - Change config settings to try different approaches\n\n**Real-World Usage:**\nFor your own data, just update the config file with:\n- Your CSV file path\n- Which columns represent protected groups\n- How much bias correction to apply\n- Your fairness goals\n\nThen run `python run_pipeline.py config.yml` and the system handles everything else.\n\n**The Bottom Line:**\nFairness in AI doesn't happen by accident. It requires systematic measurement, correction, and verification. This toolkit makes that process simple and repeatable."
  },
  {
   "cell_type": "markdown",
   "source": "## Command Line Usage\n\nOur toolkit can also be run directly from the command line using the orchestrator script:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import subprocess\nimport os\n\nprint(\"Running the fairness pipeline from command line:\")\nprint(\"Command: python run_pipeline.py config.yml\")\nprint(\"\\nThis demonstrates the main orchestrator script that:\")\nprint(\"1. Parses the config.yml file\")\nprint(\"2. Executes the three-step fairness workflow\")\nprint(\"3. Logs results to MLflow\")\n\n# Change to parent directory and run the pipeline\noriginal_dir = os.getcwd()\nos.chdir(parent_dir)\n\ntry:\n    result = subprocess.run([\n        'python', 'run_pipeline.py', 'config.yml'\n    ], capture_output=True, text=True, timeout=120)\n    \n    print(f\"\\nExit code: {result.returncode}\")\n    if result.returncode == 0:\n        print(\"✓ Pipeline executed successfully via command line\")\n    else:\n        print(\"✗ Pipeline execution failed\")\n        print(\"Error output:\", result.stderr[:500])\n        \n    # Show key output lines\n    if result.stdout:\n        lines = result.stdout.split('\\n')\n        key_lines = [line for line in lines if any(keyword in line.lower() for keyword in \n                    ['loading', 'completed', 'improvement', 'accuracy', 'violation'])]\n        if key_lines:\n            print(\"\\nKey output:\")\n            for line in key_lines[:10]:  # Show first 10 relevant lines\n                print(f\"  {line}\")\n                \nexcept subprocess.TimeoutExpired:\n    print(\"✗ Pipeline execution timed out\")\nexcept Exception as e:\n    print(f\"✗ Error running pipeline: {e}\")\nfinally:\n    os.chdir(original_dir)\n\nprint(f\"\\nThis shows how the integrated system works:\")\nprint(f\"- Single command runs the entire fairness pipeline\")\nprint(f\"- No need to manually coordinate between modules\")\nprint(f\"- Config file controls all behavior\")\nprint(f\"- Results automatically logged to MLflow\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## How This Addresses the Integration Challenge\n\nThis notebook demonstrates the solution to the key problem outlined in the task: **transforming individual fairness modules into an integrated, orchestrated system**.\n\n### The Problem We Solved\n- **Before**: Separate bias detection, data transformation, and fair training modules\n- **After**: A unified pipeline that coordinates all three automatically\n\n### Our Integration Approach\n\n**1. Central Configuration (`config.yml`):**\n- Single file controls the entire fairness workflow\n- Specifies which transformer to use (BiasMitigationTransformer)\n- Defines the training method (FairnessConstrainedClassifier)\n- Sets fairness goals and thresholds\n\n**2. Pipeline Orchestrator (`run_pipeline.py` via PipelineExecutor):**\n- Parses the config file automatically\n- Executes the three-step workflow:\n  - Step 1: Baseline measurement using MeasurementModule\n  - Step 2: Data transformation and model training\n  - Step 3: Final validation and comparison\n\n**3. MLOps Integration:**\n- Automatic logging to MLflow\n- Tracks metrics, models, and configurations\n- Enables reproducibility and comparison\n\n### Why This Matters for Scale\nThis integrated approach solves the organizational challenge:\n- **Consistency**: Every team uses the same workflow\n- **Reproducibility**: Experiments can be replicated exactly\n- **Traceability**: Full audit trail of what was done\n- **Flexibility**: Easy to adjust parameters without code changes\n\nThe notebook shows how individual modules become a production-ready system through proper orchestration.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## What We Accomplished\n\nThis demo showed how to systematically detect and reduce bias in machine learning models by **integrating individual fairness modules into a cohesive pipeline**.\n\n### The Three-Step Integration Process:\n\n**Step 1: Baseline Measurement (MeasurementModule)**\n- Used BiasDetector to audit raw data and baseline model\n- Identified specific bias patterns and fairness violations\n- Established metrics to track improvement\n\n**Step 2: Transform Data and Train Model (Pipeline + Training Modules)**\n- Applied BiasMitigationTransformer with 80% repair level (from config)\n- Trained FairnessConstrainedClassifier with demographic parity constraints\n- Coordinated data flow between transformation and training\n\n**Step 3: Final Validation (MeasurementModule)**\n- Re-measured fairness with the same BiasDetector\n- Compared baseline vs final results\n- Generated improvement report\n\n### Key Integration Benefits:\n- **Declarative Configuration**: Single YAML file controls entire workflow\n- **Module Orchestration**: Automated coordination between measurement, pipeline, and training components\n- **MLOps Integration**: Automatic logging of metrics, models, and configs to MLflow\n- **Reproducible Workflows**: Anyone can replicate results using the same config\n\n### From Modules to Production System:\n- **Before**: Individual components solving point problems\n- **After**: Integrated toolkit that scales across teams and projects\n- **Result**: Systematic, reproducible fairness workflows\n\n### Real-World Usage:\nFor your own data, the integration is simple:\n1. Update `config.yml` with your data path and fairness requirements\n2. Run `python run_pipeline.py config.yml` \n3. View results in MLflow UI\n\nThis demonstrates how proper integration transforms individual fairness modules into a production-ready system that can be deployed organization-wide."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}