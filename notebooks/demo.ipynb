{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness Pipeline Toolkit - Complete Demonstration\n",
    "\n",
    "This notebook provides a comprehensive walkthrough of the Fairness Pipeline Development Toolkit, demonstrating how to:\n",
    "\n",
    "1. Configure and run fairness-aware ML pipelines\n",
    "2. Analyze bias detection results\n",
    "3. Apply bias mitigation techniques\n",
    "4. Train fairness-constrained models\n",
    "5. Evaluate improvements using MLflow tracking\n",
    "6. Interpret results and make data-driven decisions\n",
    "\n",
    "## Overview\n",
    "\n",
    "The toolkit implements a three-step fairness pipeline:\n",
    "\n",
    "**Step 1: Baseline Measurement** - Analyze raw data for bias and fairness violations  \n",
    "**Step 2: Data Processing & Training** - Apply bias mitigation and train fair models  \n",
    "**Step 3: Final Validation** - Compare results and generate improvement reports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's ensure all dependencies are installed and import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport warnings\nfrom fairness_pipeline_toolkit.pipeline_executor import PipelineExecutor\nfrom fairness_pipeline_toolkit.config import ConfigParser\n\nwarnings.filterwarnings('ignore')\n\n# Add parent directory to path for imports\nparent_dir = Path().resolve().parent\nsrc_path = parent_dir / \"src\"\nsys.path.insert(0, str(src_path))\n\n# Configure plotting\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\nprint(\"âœ… Environment setup complete!\")\nprint(f\"ğŸ“ Working directory: {Path().resolve()}\")\nprint(f\"ğŸ“¦ Source path: {src_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Overview\n",
    "\n",
    "Let's examine and understand our pipeline configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the configuration\n",
    "config_path = parent_dir / \"config.yml\"\n",
    "config = ConfigParser.load(config_path)\n",
    "\n",
    "print(\"ğŸ“‹ PIPELINE CONFIGURATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"ğŸ”µ Data Configuration:\")\n",
    "for key, value in config['data'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nğŸŸ¡ Preprocessing Configuration:\")\n",
    "transformer_config = config['preprocessing']['transformer']\n",
    "print(f\"  Transformer: {transformer_config['name']}\")\n",
    "for param, value in transformer_config['parameters'].items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\nğŸŸ¢ Training Configuration:\")\n",
    "training_config = config['training']['method']\n",
    "print(f\"  Method: {training_config['name']}\")\n",
    "for param, value in training_config['parameters'].items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\nğŸ”´ Evaluation Configuration:\")\n",
    "eval_config = config['evaluation']\n",
    "print(f\"  Primary Metric: {eval_config['primary_metric']}\")\n",
    "print(f\"  Fairness Threshold: {eval_config['fairness_threshold']}\")\n",
    "print(f\"  Additional Metrics: {', '.join(eval_config['additional_metrics'])}\")\n",
    "\n",
    "print(\"\\nğŸŸ£ MLflow Configuration:\")\n",
    "mlflow_config = config['mlflow']\n",
    "print(f\"  Experiment Name: {mlflow_config['experiment_name']}\")\n",
    "print(f\"  Log Model: {mlflow_config['log_model']}\")\n",
    "print(f\"  Log Config: {mlflow_config['log_config']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding the Synthetic Dataset\n",
    "\n",
    "Since no external dataset is provided, our pipeline generates synthetic data with intentional bias. Let's explore this data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline executor to access data generation\n",
    "executor = PipelineExecutor(config, verbose=False)\n",
    "\n",
    "# Generate synthetic data for exploration\n",
    "synthetic_data = executor._generate_synthetic_data(n_samples=1000)\n",
    "\n",
    "print(\"ğŸ“Š SYNTHETIC DATASET OVERVIEW\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset shape: {synthetic_data.shape}\")\n",
    "print(\"\\nColumn types:\")\n",
    "for col in synthetic_data.columns:\n",
    "    print(f\"  {col}: {synthetic_data[col].dtype}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nğŸ“‹ Sample Data:\")\n",
    "display(synthetic_data.head(10))\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nğŸ“ˆ Descriptive Statistics:\")\n",
    "display(synthetic_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Bias in the Synthetic Data\n",
    "\n",
    "Let's visualize the intentional bias built into our synthetic dataset:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create visualizations to show bias\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Target rate by race\n",
    "race_target = synthetic_data.groupby('race')['target'].mean().sort_values(ascending=False)\n",
    "race_target.plot(kind='bar', ax=axes[0,0], title='Target Rate by Race', color='skyblue')\n",
    "axes[0,0].set_ylabel('Positive Rate')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Target rate by sex\n",
    "sex_target = synthetic_data.groupby('sex')['target'].mean().sort_values(ascending=False)\n",
    "sex_target.plot(kind='bar', ax=axes[0,1], title='Target Rate by Sex', color='lightcoral')\n",
    "axes[0,1].set_ylabel('Positive Rate')\n",
    "axes[0,1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Income distribution by race\n",
    "synthetic_data.boxplot(column='income', by='race', ax=axes[1,0])\n",
    "axes[1,0].set_title('Income Distribution by Race')\n",
    "axes[1,0].set_xlabel('Race')\n",
    "axes[1,0].set_ylabel('Income')\n",
    "\n",
    "# Education distribution by sex\n",
    "synthetic_data.boxplot(column='education_years', by='sex', ax=axes[1,1])\n",
    "axes[1,1].set_title('Education Years by Sex')\n",
    "axes[1,1].set_xlabel('Sex')\n",
    "axes[1,1].set_ylabel('Education Years')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Bias Visualization in Synthetic Dataset', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Print bias summary\n",
    "print(\"\\nâš ï¸  INTENTIONAL BIAS SUMMARY:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Overall target rate: {synthetic_data['target'].mean():.3f}\")\n",
    "print(\"\\nTarget rate by race:\")\n",
    "for race, rate in race_target.items():\n",
    "    print(f\"  {race}: {rate:.3f}\")\n",
    "print(f\"  Max difference: {race_target.max() - race_target.min():.3f}\")\n",
    "\n",
    "print(\"\\nTarget rate by sex:\")\n",
    "for sex, rate in sex_target.items():\n",
    "    print(f\"  {sex}: {rate:.3f}\")\n",
    "print(f\"  Difference: {abs(sex_target.iloc[0] - sex_target.iloc[1]):.3f}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Running the Complete Fairness Pipeline\n",
    "\n",
    "Now let's execute the complete pipeline and observe each step in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = PipelineExecutor(config, verbose=True)\n",
    "\n",
    "results = executor.execute_pipeline()\n",
    "\n",
    "print(\"\\nâœ… Pipeline execution completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detailed Analysis of Results\n",
    "\n",
    "Let's dive deeper into the results and understand what happened at each step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Baseline Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract baseline results\n",
    "baseline_report = results['baseline_report']['prediction_audit']\n",
    "baseline_metrics = baseline_report['metrics']\n",
    "\n",
    "print(\"ğŸ¯ Performance Metrics:\")\n",
    "performance_metrics = ['accuracy', 'precision', 'recall']\n",
    "for metric in performance_metrics:\n",
    "    if metric in baseline_metrics:\n",
    "        print(f\"  {metric.title()}: {baseline_metrics[metric]:.4f}\")\n",
    "\n",
    "print(\"\\nâš–ï¸  Fairness Metrics:\")\n",
    "fairness_metrics = ['demographic_parity_difference', 'equalized_odds_difference']\n",
    "for metric in fairness_metrics:\n",
    "    if metric in baseline_metrics:\n",
    "        value = baseline_metrics[metric]\n",
    "        threshold = config['evaluation']['fairness_threshold']\n",
    "        status = \"âŒ VIOLATION\" if value > threshold else \"âœ… OK\"\n",
    "        print(f\"  {metric.replace('_', ' ').title()}: {value:.4f} ({status})\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Overall Fairness Score: {baseline_report['overall_fairness_score']:.4f}\")\n",
    "\n",
    "# Violations summary\n",
    "violations = baseline_report['fairness_violations']\n",
    "if any(violations.values()):\n",
    "    print(\"\\nâš ï¸  DETECTED VIOLATIONS:\")\n",
    "    for violation, detected in violations.items():\n",
    "        if detected:\n",
    "            print(f\"  - {violation.replace('_', ' ').title()}\")\n",
    "else:\n",
    "    print(\"\\nâœ… No fairness violations detected in baseline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Final Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract final results\n",
    "final_report = results['final_report']\n",
    "final_metrics = final_report['metrics']\n",
    "\n",
    "print(\"ğŸ¯ FINAL MODEL PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"ğŸ¯ Performance Metrics:\")\n",
    "for metric in performance_metrics:\n",
    "    if metric in final_metrics:\n",
    "        print(f\"  {metric.title()}: {final_metrics[metric]:.4f}\")\n",
    "\n",
    "print(\"\\nâš–ï¸  Fairness Metrics:\")\n",
    "for metric in fairness_metrics:\n",
    "    if metric in final_metrics:\n",
    "        value = final_metrics[metric]\n",
    "        threshold = config['evaluation']['fairness_threshold']\n",
    "        status = \"âŒ VIOLATION\" if value > threshold else \"âœ… OK\"\n",
    "        print(f\"  {metric.replace('_', ' ').title()}: {value:.4f} ({status})\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Overall Fairness Score: {final_report['overall_fairness_score']:.4f}\")\n",
    "\n",
    "# Violations summary\n",
    "final_violations = final_report['fairness_violations']\n",
    "if any(final_violations.values()):\n",
    "    print(\"\\nâš ï¸  REMAINING VIOLATIONS:\")\n",
    "    for violation, detected in final_violations.items():\n",
    "        if detected:\n",
    "            print(f\"  - {violation.replace('_', ' ').title()}\")\n",
    "else:\n",
    "    print(\"\\nâœ… No fairness violations in final model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Improvement Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create improvement comparison visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Performance metrics comparison\n",
    "perf_comparison = pd.DataFrame({\n",
    "    'Baseline': [baseline_metrics[m] for m in performance_metrics if m in baseline_metrics],\n",
    "    'Final': [final_metrics[m] for m in performance_metrics if m in final_metrics]\n",
    "}, index=[m.title() for m in performance_metrics if m in baseline_metrics])\n",
    "\n",
    "perf_comparison.plot(kind='bar', ax=axes[0], title='Performance Metrics Comparison')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].legend()\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Fairness metrics comparison\n",
    "fair_comparison = pd.DataFrame({\n",
    "    'Baseline': [baseline_metrics[m] for m in fairness_metrics if m in baseline_metrics],\n",
    "    'Final': [final_metrics[m] for m in fairness_metrics if m in final_metrics]\n",
    "}, index=[m.replace('_', ' ').title() for m in fairness_metrics if m in baseline_metrics])\n",
    "\n",
    "fair_comparison.plot(kind='bar', ax=axes[1], title='Fairness Metrics Comparison', color=['red', 'green'])\n",
    "axes[1].set_ylabel('Difference Score')\n",
    "axes[1].axhline(y=config['evaluation']['fairness_threshold'], color='orange', linestyle='--', label='Fairness Threshold')\n",
    "axes[1].legend()\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed improvement analysis\n",
    "print(\"\\nğŸ“Š DETAILED IMPROVEMENT ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "primary_metric = config['evaluation']['primary_metric']\n",
    "baseline_primary = baseline_metrics[primary_metric]\n",
    "final_primary = final_metrics[primary_metric]\n",
    "improvement = baseline_primary - final_primary\n",
    "improvement_pct = (improvement / baseline_primary) * 100 if baseline_primary != 0 else 0\n",
    "\n",
    "print(f\"ğŸ¯ Primary Fairness Metric ({primary_metric.replace('_', ' ').title()}):\")\n",
    "print(f\"  Baseline: {baseline_primary:.4f}\")\n",
    "print(f\"  Final: {final_primary:.4f}\")\n",
    "print(f\"  Absolute Improvement: {improvement:.4f}\")\n",
    "print(f\"  Percentage Improvement: {improvement_pct:.1f}%\")\n",
    "\n",
    "# Performance trade-offs\n",
    "print(\"\\nâš–ï¸  Performance Trade-offs:\")\n",
    "for metric in performance_metrics:\n",
    "    if metric in baseline_metrics and metric in final_metrics:\n",
    "        baseline_val = baseline_metrics[metric]\n",
    "        final_val = final_metrics[metric]\n",
    "        change = final_val - baseline_val\n",
    "        change_pct = (change / baseline_val) * 100 if baseline_val != 0 else 0\n",
    "        direction = \"ğŸ“ˆ Improved\" if change > 0 else \"ğŸ“‰ Decreased\" if change < 0 else \"â¡ï¸  Unchanged\"\n",
    "        print(f\"  {metric.title()}: {baseline_val:.4f} â†’ {final_val:.4f} ({change:+.4f}, {change_pct:+.1f}%) {direction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding the Transformations\n",
    "\n",
    "Let's examine what the bias mitigation transformer actually did to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transformation details\n",
    "transformer = results['transformer']\n",
    "if transformer:\n",
    "    transformation_info = transformer.get_mitigation_details()\n",
    "    \n",
    "    print(\"ğŸ”§ BIAS MITIGATION TRANSFORMATION DETAILS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"Repair Level: {transformation_info['repair_level']}\")\n",
    "    print(f\"Sensitive Features: {transformation_info['sensitive_features']}\")\n",
    "    print(f\"Non-sensitive Features: {transformation_info['non_sensitive_features']}\")\n",
    "    \n",
    "    print(\"\\nğŸ“Š Group Statistics:\")\n",
    "    for sensitive_attr, group_stats in transformation_info['group_statistics'].items():\n",
    "        print(f\"\\n  {sensitive_attr}:\")\n",
    "        for group, stats in group_stats.items():\n",
    "            print(f\"    {group}: {stats['size']} samples\")\n",
    "            for feature, mean_val in stats['mean'].items():\n",
    "                overall_mean = transformation_info['overall_means'][feature]\n",
    "                diff = overall_mean - mean_val\n",
    "                print(f\"      {feature}: {mean_val:.3f} (diff from overall: {diff:+.3f})\")\n",
    "else:\n",
    "    print(\"âŒ No transformer found in results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Information and Fairness Constraints\n",
    "\n",
    "Let's examine the fairness-constrained model that was trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model details\n",
    "model = results['model']\n",
    "if model:\n",
    "    fairness_info = model.get_fairness_info()\n",
    "    \n",
    "    print(\"ğŸ¤– FAIRNESS-CONSTRAINED MODEL DETAILS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for key, value in fairness_info.items():\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "    \n",
    "    # Show whether fairlearn is being used\n",
    "    if hasattr(model, 'use_fairlearn'):\n",
    "        if model.use_fairlearn:\n",
    "            print(\"\\nâœ… Using Fairlearn's ExponentiatedGradient for constraint optimization\")\n",
    "        else:\n",
    "            print(\"\\nâš ï¸  Using fallback fair classifier (Fairlearn not available)\")\n",
    "            if hasattr(model, 'group_thresholds_'):\n",
    "                print(\"\\nğŸ“Š Group-specific Decision Thresholds:\")\n",
    "                for attr, thresholds in model.group_thresholds_.items():\n",
    "                    print(f\"  {attr}:\")\n",
    "                    for group, threshold in thresholds.items():\n",
    "                        print(f\"    {group}: {threshold:.4f}\")\n",
    "else:\n",
    "    print(\"âŒ No model found in results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. MLflow Experiment Tracking\n",
    "\n",
    "Our results have been automatically logged to MLflow. Let's explore what was captured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Get the experiment\n",
    "experiment_name = config['mlflow']['experiment_name']\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "if experiment:\n",
    "    print(f\"ğŸ”¬ MLFLOW EXPERIMENT: {experiment_name}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Experiment ID: {experiment.experiment_id}\")\n",
    "    print(f\"Artifact Location: {experiment.artifact_location}\")\n",
    "    \n",
    "    # Get recent runs\n",
    "    client = MlflowClient()\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        max_results=5,\n",
    "        order_by=[\"created_time DESC\"]\n",
    "    )\n",
    "    \n",
    "    if runs:\n",
    "        latest_run = runs[0]\n",
    "        print(f\"\\nğŸ“Š Latest Run ID: {latest_run.info.run_id}\")\n",
    "        print(f\"Status: {latest_run.info.status}\")\n",
    "        print(f\"Start Time: {pd.to_datetime(latest_run.info.start_time, unit='ms')}\")\n",
    "        \n",
    "        # Show logged metrics\n",
    "        print(\"\\nğŸ“ˆ Logged Metrics:\")\n",
    "        for metric_name, metric_value in latest_run.data.metrics.items():\n",
    "            print(f\"  {metric_name}: {metric_value:.4f}\")\n",
    "        \n",
    "        # Show artifacts\n",
    "        artifacts = client.list_artifacts(latest_run.info.run_id)\n",
    "        print(\"\\nğŸ“ Logged Artifacts:\")\n",
    "        for artifact in artifacts:\n",
    "            if artifact.is_dir:\n",
    "                print(f\"  ğŸ“ {artifact.path}/\")\n",
    "                # List contents of directory\n",
    "                sub_artifacts = client.list_artifacts(latest_run.info.run_id, artifact.path)\n",
    "                for sub_artifact in sub_artifacts[:3]:  # Show first 3 items\n",
    "                    print(f\"    ğŸ“„ {sub_artifact.path}\")\n",
    "                if len(sub_artifacts) > 3:\n",
    "                    print(f\"    ... and {len(sub_artifacts) - 3} more files\")\n",
    "            else:\n",
    "                print(f\"  ğŸ“„ {artifact.path}\")\n",
    "        \n",
    "        # Show tags\n",
    "        if latest_run.data.tags:\n",
    "            print(\"\\nğŸ·ï¸  Tags:\")\n",
    "            for tag_key, tag_value in latest_run.data.tags.items():\n",
    "                if not tag_key.startswith('mlflow.'):\n",
    "                    print(f\"  {tag_key}: {tag_value}\")\n",
    "    \n",
    "    print(\"\\nğŸŒ To view in MLflow UI, run: mlflow ui\")\n",
    "    print(f\"ğŸ“‚ In the parent directory: {parent_dir}\")\n",
    "else:\n",
    "    print(f\"âŒ Experiment '{experiment_name}' not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comprehensive Results Summary\n",
    "\n",
    "Let's create a final comprehensive summary of our fairness pipeline results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ FAIRNESS PIPELINE RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configuration summary\n",
    "print(\"âš™ï¸  Configuration Used:\")\n",
    "print(f\"  â€¢ Data: Synthetic dataset ({config['data']['input_path']})\")\n",
    "print(f\"  â€¢ Sensitive Features: {', '.join(config['data']['sensitive_features'])}\")\n",
    "print(f\"  â€¢ Bias Mitigation: {config['preprocessing']['transformer']['name']} (repair_level={config['preprocessing']['transformer']['parameters']['repair_level']})\")\n",
    "print(f\"  â€¢ Fair Training: {config['training']['method']['name']} ({config['training']['method']['parameters']['constraint']})\")\n",
    "print(f\"  â€¢ Primary Metric: {config['evaluation']['primary_metric']}\")\n",
    "print(f\"  â€¢ Fairness Threshold: {config['evaluation']['fairness_threshold']}\")\n",
    "\n",
    "# Results summary\n",
    "print(\"\\nğŸ“Š Key Results:\")\n",
    "\n",
    "# Fairness improvements\n",
    "primary_metric = config['evaluation']['primary_metric']\n",
    "if primary_metric in baseline_metrics and primary_metric in final_metrics:\n",
    "    baseline_val = baseline_metrics[primary_metric]\n",
    "    final_val = final_metrics[primary_metric]\n",
    "    improvement = baseline_val - final_val\n",
    "    improvement_pct = (improvement / baseline_val) * 100 if baseline_val != 0 else 0\n",
    "    \n",
    "    threshold = config['evaluation']['fairness_threshold']\n",
    "    baseline_violation = \"Yes\" if baseline_val > threshold else \"No\"\n",
    "    final_violation = \"Yes\" if final_val > threshold else \"No\"\n",
    "    \n",
    "    print(f\"\\n  ğŸ¯ Primary Fairness Metric ({primary_metric.replace('_', ' ').title()}):\")\n",
    "    print(f\"    Baseline: {baseline_val:.4f} (Violation: {baseline_violation})\")\n",
    "    print(f\"    Final: {final_val:.4f} (Violation: {final_violation})\")\n",
    "    print(f\"    Improvement: {improvement:.4f} ({improvement_pct:+.1f}%)\")\n",
    "\n",
    "# Performance summary\n",
    "print(\"\\n  ğŸ“ˆ Performance Summary:\")\n",
    "for metric in ['accuracy', 'precision', 'recall']:\n",
    "    if metric in baseline_metrics and metric in final_metrics:\n",
    "        baseline_val = baseline_metrics[metric]\n",
    "        final_val = final_metrics[metric]\n",
    "        change = final_val - baseline_val\n",
    "        symbol = \"ğŸ“ˆ\" if change > 0 else \"ğŸ“‰\" if change < 0 else \"â¡ï¸\"\n",
    "        print(f\"    {metric.title()}: {baseline_val:.4f} â†’ {final_val:.4f} ({change:+.4f}) {symbol}\")\n",
    "\n",
    "# Overall assessment\n",
    "baseline_fairness_score = baseline_report['overall_fairness_score']\n",
    "final_fairness_score = final_report['overall_fairness_score']\n",
    "fairness_improvement = final_fairness_score - baseline_fairness_score\n",
    "\n",
    "print(\"\\n  ğŸ† Overall Fairness Score:\")\n",
    "print(f\"    Baseline: {baseline_fairness_score:.4f}\")\n",
    "print(f\"    Final: {final_fairness_score:.4f}\")\n",
    "print(f\"    Improvement: {fairness_improvement:+.4f}\")\n",
    "\n",
    "# Success criteria\n",
    "print(\"\\nâœ… Success Assessment:\")\n",
    "success_count = 0\n",
    "total_criteria = 4\n",
    "\n",
    "# Criterion 1: Reduced primary fairness metric\n",
    "if improvement > 0:\n",
    "    print(f\"  âœ… Primary fairness metric improved by {improvement:.4f}\")\n",
    "    success_count += 1\n",
    "else:\n",
    "    print(f\"  âŒ Primary fairness metric did not improve ({improvement:.4f})\")\n",
    "\n",
    "# Criterion 2: No severe performance degradation (< 10% decrease)\n",
    "accuracy_change = final_metrics['accuracy'] - baseline_metrics['accuracy']\n",
    "accuracy_change_pct = (accuracy_change / baseline_metrics['accuracy']) * 100\n",
    "if accuracy_change_pct > -10:\n",
    "    print(f\"  âœ… Accuracy maintained within acceptable range ({accuracy_change_pct:+.1f}%)\")\n",
    "    success_count += 1\n",
    "else:\n",
    "    print(f\"  âŒ Accuracy degraded significantly ({accuracy_change_pct:+.1f}%)\")\n",
    "\n",
    "# Criterion 3: Final model meets fairness threshold\n",
    "if final_val <= threshold:\n",
    "    print(f\"  âœ… Final model meets fairness threshold ({final_val:.4f} â‰¤ {threshold})\")\n",
    "    success_count += 1\n",
    "else:\n",
    "    print(f\"  âš ï¸  Final model still exceeds fairness threshold ({final_val:.4f} > {threshold})\")\n",
    "\n",
    "# Criterion 4: Successful MLflow logging\n",
    "if experiment:\n",
    "    print(f\"  âœ… Results successfully logged to MLflow experiment '{experiment_name}'\")\n",
    "    success_count += 1\n",
    "else:\n",
    "    print(\"  âŒ Failed to log results to MLflow\")\n",
    "\n",
    "# Overall success\n",
    "success_rate = (success_count / total_criteria) * 100\n",
    "print(f\"\\nğŸ¯ Overall Success Rate: {success_count}/{total_criteria} ({success_rate:.0f}%)\")\n",
    "\n",
    "if success_rate >= 75:\n",
    "    print(\"ğŸŒŸ EXCELLENT: Fairness pipeline achieved strong results!\")\n",
    "elif success_rate >= 50:\n",
    "    print(\"ğŸ‘ GOOD: Fairness pipeline achieved satisfactory results\")\n",
    "else:\n",
    "    print(\"âš ï¸  NEEDS IMPROVEMENT: Consider tuning parameters or trying different approaches\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ FAIRNESS PIPELINE DEMONSTRATION COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps and Recommendations\n",
    "\n",
    "Based on the results of this demonstration, here are key insights and recommendations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”§ **Configuration Tuning Recommendations**\n",
    "\n",
    "1. **Repair Level Optimization**: Try different `repair_level` values (0.1 to 1.0) to find the optimal balance between fairness and performance\n",
    "\n",
    "2. **Constraint Selection**: Experiment with different fairness constraints:\n",
    "   - `demographic_parity`: Ensures equal positive prediction rates across groups\n",
    "   - `equalized_odds`: Ensures equal true/false positive rates across groups\n",
    "\n",
    "3. **Base Estimator Tuning**: Consider using different base estimators or tuning hyperparameters for better convergence\n",
    "\n",
    "### ğŸ“Š **Monitoring and Production Considerations**\n",
    "\n",
    "1. **Continuous Monitoring**: Set up automated fairness monitoring in production\n",
    "2. **Data Drift Detection**: Monitor for changes in data distribution that might affect fairness\n",
    "3. **A/B Testing**: Compare fairness-constrained models against baseline models in production\n",
    "4. **Stakeholder Review**: Regular review of fairness metrics with domain experts\n",
    "\n",
    "### ğŸš€ **Advanced Usage**\n",
    "\n",
    "1. **Custom Datasets**: Replace synthetic data with real-world datasets\n",
    "2. **Multiple Sensitive Attributes**: Experiment with intersectional fairness\n",
    "3. **Custom Transformers**: Implement domain-specific bias mitigation techniques\n",
    "4. **Advanced Constraints**: Implement individual fairness or counterfactual fairness constraints\n",
    "\n",
    "### ğŸ“š **Learning Resources**\n",
    "\n",
    "- **Fairlearn Documentation**: https://fairlearn.org/\n",
    "- **MLflow Documentation**: https://mlflow.org/docs/latest/index.html\n",
    "- **Fairness in ML Research**: Explore recent papers on algorithmic fairness\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for exploring the Fairness Pipeline Toolkit! ğŸ‰**\n",
    "\n",
    "This toolkit provides a solid foundation for implementing fairness-aware machine learning in production environments. Remember that fairness is an ongoing process that requires continuous monitoring, evaluation, and improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
